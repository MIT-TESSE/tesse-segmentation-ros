{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "Distribution authorized to U.S. Government agencies and their contractors. Other requests for this document shall be referred to the MIT Lincoln Laboratory Technology Office.\n",
    "\n",
    "This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering.\n",
    "\n",
    "Â© 2019 Massachusetts Institute of Technology.\n",
    "\n",
    "The software/firmware is provided to you on an As-Is basis\n",
    "\n",
    "Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Segmentation Model\n",
    "\n",
    "### Contents\n",
    "- [Configuration](#Configuration)\n",
    "- [Define Model](#Define-Model)\n",
    "- [Train Model](#Train-Model)\n",
    "- [Visualize Performance](#Visualize-Performance)\n",
    "- [Export Model to ONNX](#Export-Model-to-ONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tesse_semantic_segmentation.data import TESSEDataset\n",
    "from tesse_semantic_segmentation.utils import GOSEEK_CLASSES, CrossEntropy\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_CLASSES = len(GOSEEK_CLASSES)\n",
    "EPOCHS = 20\n",
    "IMG_RESOLUTION = (256, 320)\n",
    "RGB_IMG_DIR = \"data/goseek-v0.1.0-v2/rgb/\"\n",
    "SEGMENTATION_IMG_DIR = \"data/goseek-v0.1.0-v2/segmentation/\"\n",
    "\n",
    "log_dir = Path(\"./goseek-v0.1.0-weights-v2\")\n",
    "log_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = sorted(Path(RGB_IMG_DIR).glob(\"*png\"))\n",
    "labels = sorted(Path(SEGMENTATION_IMG_DIR).glob(\"*png\"))\n",
    "\n",
    "training_scenes = (1, )\n",
    "validation_scenes = (2,)\n",
    "\n",
    "train_imgs = [img for img in imgs if int(img.stem[-1]) in training_scenes]\n",
    "train_labels = [label for label in labels if int(label.stem[-1]) in training_scenes]\n",
    "\n",
    "val_imgs = [img for img in imgs if int(img.stem[-1]) in validation_scenes]\n",
    "val_labels = [label for label in labels if int(label.stem[-1]) in validation_scenes]\n",
    "\n",
    "assert len(train_imgs) == len(train_labels) and len(val_imgs) == len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(image, label):\n",
    "    \"\"\" Preprocessor to resize images to correct resolution. \"\"\"\n",
    "    interpolation = cv2.INTER_LINEAR\n",
    "\n",
    "    # opencv flips height and width\n",
    "    image = cv2.resize(image, IMG_RESOLUTION[::-1])  # default binlinear\n",
    "    label = cv2.resize(\n",
    "        label, IMG_RESOLUTION[::-1], interpolation=cv2.INTER_NEAREST\n",
    "    )  # nearest neighbor to avoid blurring label\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for train images\n",
    "train_dataset = TESSEDataset(imgs, labels, N_CLASSES, preprocessor=preprocessor)\n",
    "\n",
    "# Dataset for validation images\n",
    "valid_dataset = TESSEDataset(val_imgs, val_labels, N_CLASSES, preprocessor=preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = \"resnet18\"\n",
    "PRETRAINED_WEIGHTS = \"imagenet\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, encoder_weights=PRETRAINED_WEIGHTS, classes=N_CLASSES\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, PRETRAINED_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=12)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_class_frequecy = train_dataset.calculate_inverse_class_frequency().to(DEVICE)\n",
    "loss = CrossEntropy(weights=inverse_class_frequecy)\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=0.0001),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, loss=loss, metrics=metrics, optimizer=optimizer, device=DEVICE, verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, loss=loss, metrics=metrics, device=DEVICE, verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "\n",
    "for i in range(0, EPOCHS):\n",
    "\n",
    "    print(\"\\nEpoch: {}\".format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    validation_logs = valid_epoch.run(valid_loader)\n",
    "\n",
    "    # save model if it's the current best\n",
    "    if max_score < validation_logs[\"iou_score\"]:\n",
    "        max_score = validation_logs[\"iou_score\"]\n",
    "        torch.save(model, f\"{log_dir}/{ENCODER}-epoch-{i+1}.pth\")\n",
    "\n",
    "    if i == 25:\n",
    "        optimizer.param_groups[0][\"lr\"] = 1e-5\n",
    "        print(\"Decrease decoder learning rate to 1e-5!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(valid_dataset))\n",
    "img, label = valid_dataset[idx]\n",
    "pred = model(torch.tensor(img[np.newaxis]).to(DEVICE))[0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(img.transpose(1, 2, 0))\n",
    "ax[1].imshow(pred.argmax(0).cpu().numpy())\n",
    "ax[2].imshow(label.argmax(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHT_PATH = \"\"\n",
    "ONNX_FILE_NAME = \"\"\n",
    "\n",
    "assert Path(MODEL_WEIGHT_PATH).exists(), \"Must give valid weight path\"\n",
    "assert (\n",
    "    ONNX_FILE_NAME and ONNX_FILE_NAME[-5:] == \".onnx\"\n",
    "), \"Must give ONNX file name with extension `.onnx`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgMaxModel(nn.Module):\n",
    "    \"\"\" Segmentation model wrapper to return 1 channel class prediction instead of \n",
    "    prediction probabilities. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_model = ArgMaxModel().to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = torch.ones((1, 3) + IMG_RESOLUTION, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(argmax_model, x_in, ONNX_FILE_NAME, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tesse-semantic-segmentation]",
   "language": "python",
   "name": "conda-env-tesse-semantic-segmentation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
